---
title: "p8105_hw5_qc2337"
output: github_document
---

Load all the package needed for this hw
```{r}
library(tidyverse)
library(rvest)
library(broom)
library(knitr)
set.seed(1)

```

## Problem 1

for a fixed group size=10, checks whether there are duplicate birthdays in the group
```{r}

birthday_sim = function(n) {

  birthdays = sample(1:365, size = n, replace = TRUE)
  
  has_duplicate = length(unique(birthdays)) < n
  
  return(has_duplicate)
}

birthday_sim(10)
```


```{r}
results = tibble(group_size = 2:50) |> 
  mutate(
    probability = map_dbl(group_size, ~mean(replicate(10000, birthday_sim(.x))))
  )

results_plot=
  results |> 
  ggplot(aes(x = group_size, y = probability)) +
  geom_line(color = "blue") +
  geom_point(color = "red") +
  labs(
    title = "Probability of Sharing a Birthday",
    x = "Group Size",
    y = "Probability"
  ) +
  theme_minimal()

print(results_plot)
```

Comment: with the group size getting larger, the higher that the probability of the people in the group sharing a birthday. 


## Problem 2

```{r}
n = 30
sigma = 5
mu = 0:6
alpha = 0.05
simulations = 5000
```


```{r}
simulate_t_test = function(mu, sigma = 5, n = 30) {
  x = rnorm(n, mean = mu, sd = sigma)
  test_result = t.test(x, mu = 0)
  tidy(test_result)
}

results = tibble(mu = rep(mu, each = simulations)) |> 
  mutate(
    sim_data = map(mu, ~simulate_t_test(.x)),
    estimate = map_dbl(sim_data, ~.x$estimate),
    p_value = map_dbl(sim_data, ~.x$p.value),
    reject_null = as.numeric(p_value < alpha)
  )

power_proportion = results |> 
  group_by(mu) |> 
  summarise(
    proportion_rejected = mean(reject_null),
    .groups = 'drop'
  )

power_proportion |> 
  ggplot(aes(x = mu, y = proportion_rejected)) +
    geom_line() +  
    geom_point() + 
    labs(
      title = "Test Power vs. True Value of Mu",
      x = "True value of mu (Effect Size)",
      y = "Power of the Test (Proportion of Null Rejected)"
  ) +
  theme_minimal()
```

Describe the association between effect size and power:
As the effect size increases, the power of the test also increases. 
Initially, there are rapid gains in power with small increases in effect size, followed by more gradual improvements until the power effectively reaches its maximum capacity.


plot showing the average estimate of ðœ‡Ì‚ on the y axis and the true value of ðœ‡on the x axis
```{r}
average_estimates = results |> 
  group_by(mu) |> 
  summarise(
    avg_estimate_all = mean(estimate),
    avg_estimate_rejected = mean(estimate[reject_null == 1]),
    .groups = 'drop'
  )

average_estimates |> 
ggplot(aes(x = mu)) +
  geom_line(aes(y = avg_estimate_all, color = "All Samples")) +
  geom_line(aes(y = avg_estimate_rejected, color = "Null Rejected"))+
  geom_point(aes(y = avg_estimate_all, color = "All Samples")) +
  geom_point(aes(y = avg_estimate_rejected, color = "Null Rejected")) +
  labs(
       title = "Average estimates of mu vs. True value of mu",
       x = "True Value of Mu",
       y = "Average Estimate of Mu",
       color = "Group"
       ) +
  theme_minimal()
```

In the sample average of ðœ‡Ì‚across tests for which the null is rejected is not  approximately equal to the true value of mu, it would overestimate the mu when having smaller effect sizes.This indicates a systematic bias where estimates from samples leading to null rejection are higher, likely due to the effect of random sampling variability where more extreme values are more likely to be observed and lead to rejection.When the effect size is large enough, the sample estimates are naturally closer to the true values.


## Problem 3

import data
```{r}
 homicide_df =  
   read_csv("data/homicide_data.csv", na = c("NA",".","")) |> 
   janitor::clean_names()
```

Describe the raw data: The "homicide_df" have 52179 observations and 12 variables.The main variables include uid, reported_date, victim_last, victim_first, victim_race, victim_age, etc.


make a new variable
```{r}
homicide_summary= 
  homicide_df |> 
  mutate(
    city_state = paste(city, state, sep = ", ")
    ) |> 
  group_by(city_state) |> 
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"), na.rm = TRUE),
    .groups = 'drop'
    )|> 
  filter(unsolved_homicides > 0)
  
kable(homicide_summary)
```
The table shows the total homicides in each city_state and total unsolved homicides in 50 large U.S. cities.


use the prop.test for the city of Baltimore, MD
```{r}
baltimore_data = 
  homicide_summary |> 
  filter(city_state == "Baltimore, MD")

unsolved = pull(baltimore_data, unsolved_homicides)
total = pull(baltimore_data, total_homicides)

prop_test_baltimore = prop.test(unsolved, total)|>
  broom::tidy()|>
  knitr::kable()
prop_test_baltimore
```

run prop.test for each of the cities
```{r}
unsolved_estimate_prop = function(unsolved, total) {
  test_result = prop.test(unsolved, total, correct = FALSE)
  broom::tidy(test_result) |> 
    select(estimate, conf.low, conf.high)
}

city_tests =
  homicide_summary |> 
  filter(total_homicides > 0) |> 
 mutate(
    prop_test_result = map2(unsolved_homicides, total_homicides, unsolved_estimate_prop)
  ) |>
  unnest(prop_test_result)

kable(city_tests)
```

Create a plot 
```{r}
city_tests |> 
ggplot(aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point(stat = "identity") + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides",
    caption = "Error bars represent 95% confidence intervals"
  ) +
  theme_minimal() 
```





